apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    app.kubernetes.io/name: kube-prometheus
    app.kubernetes.io/part-of: kube-prometheus
    prometheus: k8s
    role: alert-rules
    release: kube-prometheus-stack
  name: kube-prometheus-stack-alertmanager.rules
  namespace: monitoring
spec:
  groups:
  - name: alertmanager.rules
    rules:
    - alert: Alertmanager 重新加载失败
      annotations:
        description: '配置加载失败，影响到 {{ $labels.namespace }}/{{ $labels.pod}}。'
        runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload'
        summary: 'Alertmanager 配置重载失败。'
      expr: |-
        # 没有 max_over_time，失败的抓取可能会产生假阴性，详情见
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0
        max_over_time(alertmanager_config_last_reload_successful{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m]) == 0
      for: 10m
      labels:
        severity: critical
    - alert: Alertmanager 副本数量成员不一致
      annotations:
        description: 'Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} 只发现了 {{ $value }} 个成员，而 {{$labels.job}} 集群中应有更多的成员。'
        runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent'
        summary: 'Alertmanager 集群中的一个成员未能发现所有其他集群成员。'
      expr: |-
        # 没有 max_over_time，失败的抓取可能会产生假阴性，详情见
        # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0
        max_over_time(alertmanager_cluster_members{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m])
        < on (namespace,service) group_left
          count by (namespace,service) (max_over_time(alertmanager_cluster_members{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m]))
      for: 15m
      labels:
        severity: critical
    - alert: Alertmanager 发送警报失败
      annotations:
        description: 'Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} 向 {{ $labels.integration }} 发送通知的失败率为 {{ $value | humanizePercentage }}。'
        runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedtosendalerts'
        summary: 'Alertmanager 实例未能发送通知。'
      expr: |-
        (
          rate(alertmanager_notifications_failed_total{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m])
        /
          rate(alertmanager_notifications_total{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m])
        )
        > 0.01
      for: 5m
      labels:
        severity: critical
    - alert: Alertmanager 集群所有实例发送警报失败
      annotations:
        description: '从 {{$labels.job}} 集群中的任何实例向 {{ $labels.integration }} 发送的最小通知失败率为 {{ $value | humanizePercentage }}。'
        runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts'
        summary: '集群中的所有 Alertmanager 实例都未能向关键集成发送通知。'
      expr: |-
        min by (namespace,service, integration) (
          rate(alertmanager_notifications_failed_total{job="kube-prometheus-stack-alertmanager",namespace="monitoring", integration=~`.*`}[5m])
        /
          rate(alertmanager_notifications_total{job="kube-prometheus-stack-alertmanager",namespace="monitoring", integration=~`.*`}[5m])
        )
        > 0.01
      for: 5m
      labels:
        severity: critical
    - alert: Alertmanager 集群实例配置不一致
      annotations:
        description: '在 {{$labels.job}} 集群中的 Alertmanager 实例具有不同的配置。'
        runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent'
        summary: '同一集群中的 Alertmanager 实例具有不同的配置。'
      expr: |-
        count by (namespace,service) (
          count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job="kube-prometheus-stack-alertmanager",namespace="monitoring"})
        )
        != 1
      for: 20m
      labels:
        severity: critical
    - alert: Alertmanager 集群实例关闭
      annotations:
        description: '在 {{$labels.job}} 集群中，有 {{ $value | humanizePercentage }} 的 Alertmanager 实例在过去5分钟内启动时间少于一半。'
        runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown'
        summary: '同一集群中的一半或更多 Alertmanager 实例已关闭。'
      expr: |-
        (
          count by (namespace,service) (
            avg_over_time(up{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[5m]) < 0.5
          )
        /
          count by (namespace,service) (
            up{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}
          )
        )
        >= 0.5
      for: 5m
      labels:
        severity: critical
    - alert: Alertmanager 集群实例循环重启
      annotations:
        description: '在 {{$labels.job}} 集群中，有 {{ $value | humanizePercentage }} 的 Alertmanager 实例在过去10分钟内至少重启了5次。'
        runbook_url: 'https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping'
        summary: '同一集群中的一半或更多 Alertmanager 实例正在不断崩溃重启。'
      expr: |-
        (
          count by (namespace,service) (
            changes(process_start_time_seconds{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}[10m]) > 4
          )
        /
          count by (namespace,service) (
            up{job="kube-prometheus-stack-alertmanager",namespace="monitoring"}
          )
        )
        >= 0.5
      for: 5m
      labels:
        severity: critical
